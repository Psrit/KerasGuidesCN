{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8MVXQUFkX3n"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "IcfrhafzkZbH"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOSK2IBG6-9Z"
   },
   "source": [
    "# 使用 Keras 训练和评估\n",
    "<!--\n",
    "# Train and evaluate with Keras\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7s-G0Fajsvjn"
   },
   "source": [
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tc92ghg-sw2w"
   },
   "source": [
    "本指南包含在 TensorFlow 2.0 中训练，评估，和预测（推断）模型的内容，主要讨论两种普遍的情形：\n",
    "\n",
    "- 什么时候使用内置 API（如 `model.fit()`，`model.evaluate`，以及 `model.predict()`）来训练和验证。这个问题主要在**“使用内置的训练和评估循环”**一节中讨论。\n",
    "- 什么时候使用 eager execution 和 `GradientTape` 对象来从头编写自定义循环。这个问题在**“从头编写你自己的训练和评估循环”**一节中讨论。\n",
    "\n",
    "<!--\n",
    "This guide covers training, evaluation, and prediction (inference)  models in TensorFlow 2.0 in two broad situations:\n",
    "\n",
    "- When using built-in APIs for training & validation (such as `model.fit()`, `model.evaluate()`, `model.predict()`). This is covered in the section **\"Using built-in training & evaluation loops\"**.\n",
    "- When writing custom loops from scratch using eager execution and the `GradientTape` object. This is covered in the section **\"Writing your own training & evaluation loops from scratch\"**.\n",
    "-->\n",
    "\n",
    "一般而言，无论是在使用内置循环还是编写你自己的循环，对于每一种 Keras 模型，模型训练和评估都严格遵循同样的运行方式——这包括 Sequential 模型，使用函数式 API 构建的模型，以及通过模型派生从头编写的模型。\n",
    "<!--\n",
    "In general, whether you are using built-in loops or writing your own, model training & evaluation works strictly in the same way across every kind of Keras model -- Sequential models, models built with the Functional API, and models written from scratch via model subclassing.\n",
    "-->\n",
    "\n",
    "本指南不介绍分布式训练的内容。\n",
    "<!--\n",
    "This guide doesn't cover distributed training.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNG8OH3yuWrb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uh_b0kqEuYah"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "052DbsQ175lP"
   },
   "source": [
    "## 第一部分：使用内置的训练和评估循环\n",
    "\n",
    "当你将数据传递给模型的内置训练循环时，你应当使用 **Numpy 数组**（如果你的数据量很小，能够存储在内存中）或者 **tf.data 的 Dataset** 对象。在接下来的段落里，我们将把 MNIST 数据集表示为 Numpy 数组，来展示如何使用[优化器（optimizers）](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E4%BC%98%E5%8C%96%E5%99%A8-optimizer)，[损失函数（losses）](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E6%8D%9F%E5%A4%B1-loss)，以及[指标（metrics）](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E6%8C%87%E6%A0%87-metric)。\n",
    "<!--\n",
    "When passing data to the built-in training loops of a model, you should either use **Numpy arrays** (if your data is small and fits in memory) or **tf.data Dataset** objects. In the next few paragraphs, we'll use the MNIST dataset as Numpy arrays, in order to demonstrate how to use optimizers, losses, and metrics.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rXDWPajQ8V8V"
   },
   "source": [
    "### API 概览：一个端到端的例子\n",
    "<!--\n",
    "### API overview: a first end-to-end example\n",
    "-->\n",
    "\n",
    "让我们来看一下如下模型（这里我们使用函数式 API 来创建它，但是我们也可以使用 Sequential 模型或派生模型）：\n",
    "<!--\n",
    "Let's consider the following model (here, we build in with the Functional API, but it could be a Sequential model or a subclassed model as well):\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_eJxUppsNjMj"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C02iiuYZNklq"
   },
   "source": [
    "这就是典型的端到端工作流程的样子，它包含了训练，在[维持数据集](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E7%BB%B4%E6%8C%81%E6%95%B0%E6%8D%AE-holdout-data)（从原始训练数据中生成）上的验证，以及最终在测试数据集上的评估：\n",
    "<!--\n",
    "Here's what the typical end-to-end workflow looks like, consisting of training, validation on a holdout set generated from the original training data, and finally evaluation on the test data:\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QoRHOVNFm68"
   },
   "source": [
    "为此示例加载玩具数据集\n",
    "<!--\n",
    "Load a toy dataset for the sake of this example\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rLgOmhndFiL6"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhAyu0teFpye"
   },
   "source": [
    "指定训练配置（优化器，损失函数，指标）\n",
    "<!--\n",
    "Specify the training configuration (optimizer, loss, metrics)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YTzpaClpNvyJ"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(),  # Optimizer\n",
    "              # Loss function to minimize\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              # List of metrics to monitor\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uO0mAUQNFtL2"
   },
   "source": [
    "训练模型，将数据分成大小为[“batch_size”](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E6%89%B9%E6%AC%A1%E5%A4%A7%E5%B0%8F-batch-size)的[“批次”（“batches”）](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E6%89%B9%E6%AC%A1-batch)，并对整个数据集反复迭代若干[“周期”（“epochs”）](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E5%91%A8%E6%9C%9F-epoch)。\n",
    "<!--\n",
    "Train the model by slicing the data into \"batches\"\n",
    "of size \"batch_size\", and repeatedly iterating over\n",
    "the entire dataset for a given number of \"epochs\"\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNJjRVhNFtxe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fit model on training data\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 2s 34us/sample - loss: 0.3462 - sparse_categorical_accuracy: 0.9017 - val_loss: 0.1967 - val_sparse_categorical_accuracy: 0.9405\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.1593 - sparse_categorical_accuracy: 0.9522 - val_loss: 0.1274 - val_sparse_categorical_accuracy: 0.9647\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 26us/sample - loss: 0.1173 - sparse_categorical_accuracy: 0.9643 - val_loss: 0.1170 - val_sparse_categorical_accuracy: 0.9632\n",
      "\n",
      "history dict: {'loss': [0.34615352845668795, 0.15926761425971986, 0.11734869359135627], 'sparse_categorical_accuracy': [0.9017, 0.95216, 0.9643], 'val_loss': [0.19667511559724807, 0.1273672592371702, 0.11695939368903636], 'val_sparse_categorical_accuracy': [0.9405, 0.9647, 0.9632]}\n"
     ]
    }
   ],
   "source": [
    "print('# Fit model on training data')\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=3,\n",
    "                    # We pass some validation for\n",
    "                    # monitoring validation loss and metrics\n",
    "                    # at the end of each epoch\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WvOM2vPJF8MI"
   },
   "source": [
    "返回的“history”对象保存了训练期间损失函数和指标的记录\n",
    "<!--\n",
    "The returned \"history\" object holds a record\n",
    " of the loss values and metric values during training\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-dw2fAOF1om"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "10000/10000 [==============================] - 0s 9us/sample - loss: 0.1159 - sparse_categorical_accuracy: 0.9635\n",
      "test loss, test acc: [0.11589125097990036, 0.9635]\n",
      "\n",
      "# Generate predictions for 3 samples\n",
      "predictions:  [[ -9.204995    -7.9397      -1.3675845   -1.2221596   -9.813595\n",
      "   -4.2968235  -18.740984     8.768244    -2.6839263   -3.3358111 ]\n",
      " [ -5.959761    -2.8102214    6.0991287   -0.94537205 -15.433664\n",
      "   -3.9262004   -3.8570764   -9.060004    -2.4071803  -16.32756   ]\n",
      " [ -7.9607277    4.9811935   -3.7635896   -4.96414     -3.8687143\n",
      "   -4.852813    -5.8724465   -0.8015147   -2.8880491   -4.893906  ]]\n",
      "predictions shape: (3, 10)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(x_test[:3])\n",
    "print('predictions: ', predictions)\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R43f-GWfDkQ5"
   },
   "source": [
    "### 指定损失函数、指标和优化器\n",
    "<!--\n",
    "### Specifying a loss, metrics, and an optimizer\n",
    "-->\n",
    "\n",
    "要使用 `fit` 训练模型，你需要指定一个损失函数，一个优化器，以及（可选的）若干指标以用作性能监控。\n",
    "<!--\n",
    "To train a model with `fit`, you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n",
    "-->\n",
    "\n",
    "将这些作为 `compile()` 方法的参数传递给模型：\n",
    "<!--\n",
    "You pass these to the model as arguments to the `compile()` method:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiB85T_hM_vQ"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lq3wsghNM_7C"
   },
   "source": [
    "`metrics` 参数应当是一个列表——你的模型可以有任意数量的指标。\n",
    "<!--\n",
    "The `metrics` argument should be a list -- you model can have any number of metrics.\n",
    "-->\n",
    "\n",
    "如果你的模型有多个输出，你可以为每个输出指定不同的损失函数和指标，而且可以调整每个输出对模型总的损失函数的影响程度。你可以在**“将数据传递给多输入、多输出的模型”**一节中了解关于此方面的更多细节。\n",
    "<!--If your model has multiple outputs, your can specify  different losses and metrics for each output,\n",
    "and you can modulate the contribution of each output to the total loss of the model. You will find more details about this in the section \"**Passing data to multi-input, multi-output models**\".\n",
    "-->\n",
    "\n",
    "需要注意的是，如果优化器、损失函数以及指标的构造函数的默认设置已经足以解决问题，那么在很多情况下它们可以直接通过字符串标识符来指定（相当于一种快捷方式）：\n",
    "<!--\n",
    "Note that if you're satisfied with the default settings, in many cases the optimizer, loss, and metrics can be specified via string identifiers as a shortcut:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOQOcScsNiT3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IVHOVTqFknZP"
   },
   "source": [
    "为了方便之后重用上述代码，我们将模型的定义和编译步骤放到函数中；在本指南的不同案例里，这些函数将被多次调用：\n",
    "<!--\n",
    "For later reuse, let's put our model definition and compile step in functions; we will call them several times across different examples in this guide.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3Zrjsf_kt8k"
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1rDyyQEtC8E"
   },
   "source": [
    "#### 我们可以使用很多内置的优化器、损失函数和指标\n",
    "<!--\n",
    "#### Many built-in optimizers, losses, and metrics are available\n",
    "-->\n",
    "\n",
    "一般而言，你无须从头创建你自己的损失函数、指标以及优化器，因为你所需要的很有可能已经是 Keras API 的一部分了：\n",
    "<!--\n",
    "In general, you won't have to create from scratch your own losses, metrics, or optimizers, because what you need is likely already part of the Keras API:\n",
    "-->\n",
    "\n",
    "优化器：\n",
    "- `SGD()`（有或者没有动量）\n",
    "- `RMSprop()`\n",
    "- `Adam()`\n",
    "- 等等。\n",
    "\n",
    "<!--\n",
    "Optimizers:\n",
    "- `SGD()` (with or without momentum)\n",
    "- `RMSprop()`\n",
    "- `Adam()`\n",
    "- etc.\n",
    "-->\n",
    "\n",
    "损失函数：\n",
    "- `MeanSquaredError()`\n",
    "- `KLDivergence()`\n",
    "- `CosineSimilarity()`\n",
    "- 等等。\n",
    "\n",
    "<!--\n",
    "Losses:\n",
    "- `MeanSquaredError()`\n",
    "- `KLDivergence()`\n",
    "- `CosineSimilarity()`\n",
    "- etc.\n",
    "-->\n",
    "\n",
    "指标：\n",
    "- `AUC()`\n",
    "- `Precision()`\n",
    "- `Recall()`\n",
    "- 等等。\n",
    "\n",
    "<!--\n",
    "Metrics:\n",
    "- `AUC()`\n",
    "- `Precision()`\n",
    "- `Recall()`\n",
    "- etc.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LH1QjaltNm2U"
   },
   "source": [
    "#### 自定义损失函数\n",
    "<!--\n",
    "#### Custom losses\n",
    "-->\n",
    "\n",
    "有两种方法可以让我们使用 Keras 来自定义损失函数。其一是创建一个接收 `y_true` 和 `y_pred` 作为输入的函数。下面的例子展示了一个计算真实数据和预测值之间的绝对误差的平均值的损失函数：\n",
    "<!--\n",
    "There are two ways to provide custom losses with Keras. The first example creates a function that accepts inputs `y_true` and `y_pred`. The following example shows a loss function that computes the average absolute error between the real data and the predictions:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IIn3_iD0Nyll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 1s 29us/sample - loss: 1.3912\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.7059\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 26us/sample - loss: 0.5786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb96271710>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def basic_loss_function(y_true, y_pred):\n",
    "    return tf.math.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=basic_loss_function)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZqpsybNN_Wh"
   },
   "source": [
    "如果你需要一个可以接收除了 `y_true` 和 `y_pred` 以外其他参数的损失函数，你可以继承 `tf.keras.losses.Loss` 类，并实现以下两个方法：\n",
    "\n",
    "* `__init__(self)`：在调用损失函数时，传入的参数将被此方法接收；\n",
    "* `call(self, y_true, y_pred)`：使用目标（`y_true`）和模型预测值（`y_pred`）来计算模型的损失。\n",
    "\n",
    "<!--\n",
    "If you need a loss function that takes in parameters beside `y_true` and `y_pred`, you can subclass the `tf.keras.losses.Loss` class and implement the following two methods:\n",
    "\n",
    "* `__init__(self)` —Accept parameters to pass during the call of your loss function\n",
    "* `call(self, y_true, y_pred)` —Use the targets (`y_true`) and the model predictions (`y_pred`) to compute the model's loss\n",
    "-->\n",
    "\n",
    "传递给 `__init__()` 的参数可以在 `call()` 计算损失值时被使用。\n",
    "<!--\n",
    "Parameters passed into `__init__()` can be used during `call()` when calculating loss.\n",
    "-->\n",
    "\n",
    "下面的例子展示了如何实现一个 `WeightedCrossEntropy` 损失函数，其计算 `BinaryCrossEntropy` 损失，其中对应于某一类别的损失值乃至整个函数的损失值都能由一个标量来调节。\n",
    "<!--\n",
    "The following example shows how to implement a `WeightedCrossEntropy` loss function that calculates a `BinaryCrossEntropy` loss, where the loss of a certain class or the whole function can be modified by a scalar.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gr996axuOCfX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function binary_crossentropy in module tensorflow.python.keras.losses:\n",
      "\n",
      "binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class WeightedBinaryCrossEntropy(keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      pos_weight: Scalar to affect the positive labels of the loss function.\n",
    "      weight: Scalar to affect the entirety of the loss function.\n",
    "      from_logits: Whether to compute loss from logits or the probability.\n",
    "      reduction: Type of tf.keras.losses.Reduction to apply to loss.\n",
    "      name: Name of the loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, pos_weight, weight, from_logits=False,\n",
    "                 reduction=keras.losses.Reduction.AUTO,\n",
    "                 name='weighted_binary_crossentropy'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.pos_weight = pos_weight\n",
    "        self.weight = weight\n",
    "        self.from_logits = from_logits\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        ce = tf.losses.binary_crossentropy(\n",
    "            y_true, y_pred, from_logits=self.from_logits)[:,None]\n",
    "        ce = self.weight * (ce*(1-y_true) + self.pos_weight*ce*(y_true))\n",
    "        return ce\n",
    "    \n",
    "help(tf.losses.binary_crossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCrkoI7IyAgG"
   },
   "source": [
    "我们使用的是二元损失函数，但数据集有 10 个类别，所以我们将二元损失函数应用到数据集上，就好像模型在对每一个类别进行独立的二元分类预测。要想达到这个效果，我们需要从类别下标来构建[独热向量](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81-one-hot-encoding)：\n",
    "<!--\n",
    "Binary loss but the dataset has 10 classes, so apply the loss to the dataset as if it were making an independent binary prediction for each class. To do that, start by creating one-hot vectors from the class indices:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JCp2Xa9uqI1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function one_hot in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "one_hot(indices, depth, on_value=None, off_value=None, axis=None, dtype=None, name=None)\n",
      "    Returns a one-hot tensor.\n",
      "    \n",
      "    The locations represented by indices in `indices` take value `on_value`,\n",
      "    while all other locations take value `off_value`.\n",
      "    \n",
      "    `on_value` and `off_value` must have matching data types. If `dtype` is also\n",
      "    provided, they must be the same data type as specified by `dtype`.\n",
      "    \n",
      "    If `on_value` is not provided, it will default to the value `1` with type\n",
      "    `dtype`\n",
      "    \n",
      "    If `off_value` is not provided, it will default to the value `0` with type\n",
      "    `dtype`\n",
      "    \n",
      "    If the input `indices` is rank `N`, the output will have rank `N+1`. The\n",
      "    new axis is created at dimension `axis` (default: the new axis is appended\n",
      "    at the end).\n",
      "    \n",
      "    If `indices` is a scalar the output shape will be a vector of length `depth`\n",
      "    \n",
      "    If `indices` is a vector of length `features`, the output shape will be:\n",
      "    \n",
      "    ```\n",
      "      features x depth if axis == -1\n",
      "      depth x features if axis == 0\n",
      "    ```\n",
      "    \n",
      "    If `indices` is a matrix (batch) with shape `[batch, features]`, the output\n",
      "    shape will be:\n",
      "    \n",
      "    ```\n",
      "      batch x features x depth if axis == -1\n",
      "      batch x depth x features if axis == 1\n",
      "      depth x batch x features if axis == 0\n",
      "    ```\n",
      "    \n",
      "    If `indices` is a RaggedTensor, the 'axis' argument must be positive and refer\n",
      "    to a non-ragged axis. The output will be equivalent to applying 'one_hot' on\n",
      "    the values of the RaggedTensor, and creating a new RaggedTensor from the\n",
      "    result.\n",
      "    \n",
      "    If `dtype` is not provided, it will attempt to assume the data type of\n",
      "    `on_value` or `off_value`, if one or both are passed in. If none of\n",
      "    `on_value`, `off_value`, or `dtype` are provided, `dtype` will default to the\n",
      "    value `tf.float32`.\n",
      "    \n",
      "    Note: If a non-numeric data type output is desired (`tf.string`, `tf.bool`,\n",
      "    etc.), both `on_value` and `off_value` _must_ be provided to `one_hot`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    indices = [0, 1, 2]\n",
      "    depth = 3\n",
      "    tf.one_hot(indices, depth)  # output: [3 x 3]\n",
      "    # [[1., 0., 0.],\n",
      "    #  [0., 1., 0.],\n",
      "    #  [0., 0., 1.]]\n",
      "    \n",
      "    indices = [0, 2, -1, 1]\n",
      "    depth = 3\n",
      "    tf.one_hot(indices, depth,\n",
      "               on_value=5.0, off_value=0.0,\n",
      "               axis=-1)  # output: [4 x 3]\n",
      "    # [[5.0, 0.0, 0.0],  # one_hot(0)\n",
      "    #  [0.0, 0.0, 5.0],  # one_hot(2)\n",
      "    #  [0.0, 0.0, 0.0],  # one_hot(-1)\n",
      "    #  [0.0, 5.0, 0.0]]  # one_hot(1)\n",
      "    \n",
      "    indices = [[0, 2], [1, -1]]\n",
      "    depth = 3\n",
      "    tf.one_hot(indices, depth,\n",
      "               on_value=1.0, off_value=0.0,\n",
      "               axis=-1)  # output: [2 x 2 x 3]\n",
      "    # [[[1.0, 0.0, 0.0],   # one_hot(0)\n",
      "    #   [0.0, 0.0, 1.0]],  # one_hot(2)\n",
      "    #  [[0.0, 1.0, 0.0],   # one_hot(1)\n",
      "    #   [0.0, 0.0, 0.0]]]  # one_hot(-1)\n",
      "    \n",
      "    indices = tf.ragged.constant([[0, 1], [2]])\n",
      "    depth = 3\n",
      "    tf.one_hot(indices, depth)  # output: [2 x None x 3]\n",
      "    # [[[1., 0., 0.],\n",
      "    #   [0., 1., 0.]],\n",
      "    #  [[0., 0., 1.]]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      indices: A `Tensor` of indices.\n",
      "      depth: A scalar defining the depth of the one hot dimension.\n",
      "      on_value: A scalar defining the value to fill in output when `indices[j]\n",
      "        = i`. (default: 1)\n",
      "      off_value: A scalar defining the value to fill in output when `indices[j]\n",
      "        != i`. (default: 0)\n",
      "      axis: The axis to fill (default: -1, a new inner-most axis).\n",
      "      dtype: The data type of the output tensor.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      output: The one-hot tensor.\n",
      "    \n",
      "    Raises:\n",
      "      TypeError: If dtype of either `on_value` or `off_value` don't match `dtype`\n",
      "      TypeError: If dtype of `on_value` and `off_value` don't match one another\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_hot_y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
    "help(tf.one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piKc140AyFy5"
   },
   "source": [
    "现在使用这些独热向量以及自定义损失函数来训练模型：\n",
    "<!--\n",
    "Now use those one-hots, and the custom loss to train a model:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SeRpbezyFJh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 2s 32us/sample - loss: 0.1697\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0654\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0487\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 1s 23us/sample - loss: 0.0392\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb96288710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_uncompiled_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=WeightedBinaryCrossEntropy(\n",
    "        pos_weight=0.5, weight = 2, from_logits=True)\n",
    ")\n",
    "\n",
    "model.fit(x_train, one_hot_y_train, batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUkJi7ENDnCg"
   },
   "source": [
    "#### 自定义指标\n",
    "<!--\n",
    "#### Custom metrics\n",
    "-->\n",
    "\n",
    "如果你需要的指标不能在 API 中找到，你可以很简单地创建自定义指标，只须继承自 `Metric` 类即可。你需要实现 4 个方法：\n",
    "\n",
    "- `__init__(self)`：在这里为你的指标创建状态变量。\n",
    "- `update_state(self, y_true, y_pred, sample_weight=None)`：使用目标 `y_true` 和模型预测值 `y_pred` 来更新状态变量。\n",
    "- `result(self)`：使用状态变量来计算最终结果。\n",
    "- `reset_states(self)`：重新初始化指标的状态。\n",
    "\n",
    "<!--\n",
    "If you need a metric that isn't part of the API, you can easily create custom metrics by subclassing the `Metric` class. You will need to implement 4 methods:\n",
    "\n",
    "- `__init__(self)`,  in which you will create state variables for your metric.\n",
    "- `update_state(self, y_true, y_pred, sample_weight=None)`, which uses the targets `y_true` and the model predictions `y_pred` to update the state variables.\n",
    "- `result(self)`, which uses the state variables to compute the final results.\n",
    "- `reset_states(self)`, which reinitializes the state of the metric.\n",
    "-->\n",
    "\n",
    "状态更新和结果计算是被分开进行的（分别在 `update_state()` 和 `result()` 中完成），这是因为在一些情况下结果的计算可能非常消耗性能，故只能被周期性地计算。\n",
    "<!--\n",
    "State update and results computation are kept separate (in `update_state()` and `result()`, respectively) because in some cases, results computation might be very expensive, and would only be done periodically.\n",
    "-->\n",
    "\n",
    "下面的例子展示了如何实现一个 `CategoricalTruePositives` 指标，其统计被正确分类的样本数：\n",
    "<!--\n",
    "Here's a simple example showing how to implement a `CategoricalTruePositives` metric, that counts how many samples where correctly classified as belonging to a given class:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MbSNO6uePVsF"
   },
   "outputs": [],
   "source": [
    "class CategoricalTruePositives(keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='categorical_true_positives', **kwargs):\n",
    "        super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
    "        values = tf.cast(y_true, 'int32') == tf.cast(y_pred, 'int32')\n",
    "        values = tf.cast(values, 'float32')\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, 'float32')\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.true_positives.assign_add(tf.reduce_sum(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.true_positives\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.true_positives.assign(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGfwREANDUyo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 2s 33us/sample - loss: 0.0679 - categorical_true_positives: 49014.0000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0568 - categorical_true_positives: 49162.0000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 1s 23us/sample - loss: 0.0491 - categorical_true_positives: 49256.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffba4203e90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[CategoricalTruePositives()])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HipGsW7qDo0Q"
   },
   "source": [
    "### 处理不符合标准函数签名的损失函数和指标\n",
    "<!--\n",
    "#### Handling losses and metrics that don't fit the standard signature\n",
    "-->\n",
    "\n",
    "绝大多数损失函数和指标能由 `y_true` 和 `y_pred` 计算出来，其中 `y_pred` 是模型的一个输出值。但总是存在例外。例如，一个正则化损失函数可能只需要某层的激活函数（这种情况下不存在目标），而且该激活函数可能并不是模型的一个输出。\n",
    "<!--\n",
    "The overwhelming majority of losses and metrics can be computed from `y_true` and `y_pred`, where `y_pred` is an output of your model. But not all of them. For instance, a regularization loss may only require the activation of a layer (there are no targets in this case), and this activation may not be a model output.\n",
    "-->\n",
    "\n",
    "在这种情况下，你可以在一个自定义层的 `call` 方法内部调用 `self.add_loss(loss_value)`。在下面这个简单的例子中，模型会加上 activity regularization（注意，activity regularization 都是内置于所有 Keras 层中的——这里的 `ActivityRegularizationLayer` 层仅仅是用作演示的例子罢了）：\n",
    "<!--\n",
    "In such cases, you can call `self.add_loss(loss_value)` from inside the `call` method of a custom layer. Here's a simple example that adds activity regularization (note that activity regularization is built-in in all Keras layers -- this layer is just for the sake of providing a concrete example):\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CrYwrDR0PWab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 33us/sample - loss: 2.4935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb97108710>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
    "        return inputs  # Pass-through layer.\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "\n",
    "# The displayed loss will be much higher than before\n",
    "# due to the regularization component.\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fWnqsdwt06us"
   },
   "source": [
    "你也可以为 logging metric values 采取同样的手段：\n",
    "<!--\n",
    "You can do the same for logging metric values:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBYYhVgJ0973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 35us/sample - loss: 0.3428 - std_of_activation: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb9752e0d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MetricLoggingLayer(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # The `aggregation` argument defines\n",
    "        # how to aggregate the per-batch values\n",
    "        # over each epoch:\n",
    "        # in this case we simply average them.\n",
    "        self.add_metric(keras.backend.std(inputs),\n",
    "                        name='std_of_activation',\n",
    "                        aggregation='mean')\n",
    "        return inputs  # Pass-through layer.\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "\n",
    "# Insert std logging as a layer.\n",
    "x = MetricLoggingLayer()(x)\n",
    "\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9_9XQvA0Jap"
   },
   "source": [
    "在[函数式 API](functional.ipynb)中，你还可以调用 `model.add_loss(loss_tensor)` 或者 `model.add_metric(metric_tensor, name, aggregation)`。\n",
    "<!--\n",
    "In the [Functional API](functional.ipynb), you can also call `model.add_loss(loss_tensor)`, or `model.add_metric(metric_tensor, name, aggregation)`.\n",
    "-->\n",
    "\n",
    "下面是一个简单的例子：\n",
    "<!--\n",
    "Here's a simple example:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrA1yFql0gXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "50000/50000 [==============================] - 2s 37us/sample - loss: 2.4938 - std_of_activation: 0.0020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb97c9e690>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x1 = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x2 = layers.Dense(64, activation='relu', name='dense_2')(x1)\n",
    "outputs = layers.Dense(10, name='predictions')(x2)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
    "\n",
    "model.add_metric(keras.backend.std(x1),\n",
    "                 name='std_of_activation',\n",
    "                 aggregation='mean')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJqpjGyMAa82"
   },
   "source": [
    "#### 自动分隔出验证维持数据集\n",
    "<!--\n",
    "#### Automatically setting apart a validation holdout set\n",
    "-->\n",
    "\n",
    "在你所见到的第一个端到端的例子中，我们使用 `validation_data` 来给模型传入一个 Numpy 数组 `(x_val, y_val)`，用来在每一个训练周期的末尾计算验证损失函数和验证指标值。\n",
    "<!--\n",
    "In the first end-to-end example you saw, we used the `validation_data` argument to pass a tuple\n",
    "of Numpy arrays `(x_val, y_val)` to the model for evaluating a validation loss and validation metrics at the end of each epoch.\n",
    "-->\n",
    "\n",
    "我们还有另一种方法：参数 `validation_split` 允许我们自动地将训练数据中的一部分保留为验证数据。该参数的数值代表了原始数据中将被保留用来验证的数据的比重，因此它应当是大于 0 小于 1 的。例如，`validation_split=0.2` 表示“使用 20% 的原始数据来验证”，而 `validation_split=0.6` 表示“使用 60% 的原始数据来验证”。\n",
    "<!--\n",
    "Here's another option: the argument `validation_split` allows you to automatically reserve part of your training data for validation. The argument value represents the fraction of the data to be reserved for validation, so it should be set to a number higher than 0 and lower than 1. For instance, `validation_split=0.2` means \"use 20% of the data for validation\", and `validation_split=0.6` means \"use 60% of the data for validation\".\n",
    "-->\n",
    "\n",
    "验证数据集的选取，是通过*将 `fit` 接收到的数组（尚未打乱顺序）中最后 x% 个样本取出*得到的。\n",
    "<!--\n",
    "The way the validation is computed is by *taking the last x% samples of the arrays received by the `fit` call, before any shuffling*.\n",
    "-->\n",
    "\n",
    "You can only use `validation_split` when training with Numpy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-DQyeRePYS-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "   64/40000 [..............................] - ETA: 5:06 - loss: 2.3046 - sparse_categorical_accuracy: 0.1719 - val_loss: 2.1804 - val_sparse_categorical_accuracy: 0.2044"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb98399b10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uq0TDb15DBbc"
   },
   "source": [
    "### 使用 tf.data Dataset 来训练和评估\n",
    "<!--\n",
    "### Training & evaluation from tf.data Datasets\n",
    "-->\n",
    "\n",
    "在上面几段中，你已经知道了在传入数据为 Numpy 数组格式时，我们如何处理损失函数、指标和优化器，而且已经看到我们如何在 `fit` 中使用 `validation_data` 和 `validation_split` 参数。\n",
    "<!--\n",
    "In the past few paragraphs, you've seen how to handle losses, metrics, and optimizers, and you've seen how to use the `validation_data` and `validation_split` arguments in `fit`, when your data is passed as Numpy arrays.\n",
    "-->\n",
    "\n",
    "现在让我们来看看当数据以 tf.data Dataset 格式出现时我们该怎么处理。\n",
    "<!--\n",
    "Let's now take a look at the case where your data comes in the form of a tf.data Dataset.\n",
    "-->\n",
    "\n",
    "tf.data API 是 TensorFlow 2.0 中用于加载和预处理数据的工具集，其运行效率高，而且具有可扩展性。\n",
    "<!--\n",
    "The tf.data API is a set of utilities in TensorFlow 2.0 for loading and preprocessing data in a way that's fast and scalable.\n",
    "-->\n",
    "\n",
    "要想获取关于创建 Dataset 的完整指南，请参见 [tf.data 文档](https://www.tensorflow.org/guide/data)\n",
    "。\n",
    "<!--\n",
    "For a complete guide about creating Datasets, see [the tf.data documentation](https://www.tensorflow.org/guide/data).\n",
    "-->\n",
    "\n",
    "你可以向 `fit()`、`evaluate()` 和 `predict()` 中直接传入 Dataset 实例：\n",
    "<!--\n",
    "You can pass a Dataset instance directly to the methods `fit()`, `evaluate()`, and `predict()`:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY27EJsmENBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3297 - sparse_categorical_accuracy: 0.9070\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1551 - sparse_categorical_accuracy: 0.9535\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1137 - sparse_categorical_accuracy: 0.9661\n",
      "\n",
      "# Evaluate\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.1139 - sparse_categorical_accuracy: 0.9667\n",
      "<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'> <class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>\n"
     ]
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# First, let's create a training Dataset instance.\n",
    "# For the sake of our example, we'll use the same MNIST data as before.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Now we get a test dataset.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(64)\n",
    "\n",
    "# Since the dataset already takes care of batching,\n",
    "# we don't pass a `batch_size` argument.\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# You can also evaluate or predict on a dataset.\n",
    "print('\\n# Evaluate')\n",
    "result = model.evaluate(test_dataset)\n",
    "dict(zip(model.metrics_names, result))\n",
    "\n",
    "print(type(train_dataset), type(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r2yh2h79ENUy"
   },
   "source": [
    "注意，Dataset 在每一个训练周期结束后都会被重置，因此它们能直接为下一个训练周期所重用。\n",
    "<!--\n",
    "Note that the Dataset is reset at the end of each epoch, so it can be reused of the next epoch.\n",
    "-->\n",
    "\n",
    "如果你只想使用该 Dataset 中一定数量的批次来进行训练，你可以传递 `steps_per_epoch` 参数，该参数指定了模型将在使用这个 Dataset 训练多少步之后再开始下一次训练周期（译注：即每个周期的训练步数）。\n",
    "<!--\n",
    "If you want to run training only on a specific number of batches from this Dataset, you can pass the `steps_per_epoch` argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.\n",
    "-->\n",
    "\n",
    "如果你这样做，数据集在每一个训练周期结束后将不会被重置，相反，我们将直接取用接下来的数据批次。最终，数据集里的数据将被用完（除非它是一个无限循环数据集）。\n",
    "<!--\n",
    "If you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7t0V6K_EEdB7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100 steps\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.7820 - sparse_categorical_accuracy: 0.8025\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3218 - sparse_categorical_accuracy: 0.9108\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2461 - sparse_categorical_accuracy: 0.9319\n",
      "Help on method fit in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False, **kwargs) method of tensorflow.python.keras.engine.training.Model instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample weights)`.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of symbolic tensors, datasets,\n",
      "            generators, or `keras.utils.Sequence` instances (since they generate\n",
      "            batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            Note that the progress bar is not particularly useful when\n",
      "            logged to a file, so verbose=2 is recommended when not running\n",
      "            interactively (eg, in a production environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "              - dataset\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` could be provided.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch').\n",
      "            'batch' is a special option for dealing with the\n",
      "            limitations of HDF5 data; it shuffles in batch-sized chunks.\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample.\n",
      "            In this case you should make sure to specify\n",
      "            `sample_weight_mode=\"temporal\"` in `compile()`. This argument is not\n",
      "            supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            This argument is not supported with array inputs.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of a infinite dataset, it will run into a infinite loop.\n",
      "            If 'validation_steps' is specified and only part of the dataset\n",
      "            will be consumed, the evaluation will start from the beginning of\n",
      "            the dataset at each epoch. This ensures that the same validation\n",
      "            samples are used every time.\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "        **kwargs: Used for backwards compatibility.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: If the model was never compiled.\n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Help on method take in module tensorflow.python.data.ops.dataset_ops:\n",
      "\n",
      "take(count) method of tensorflow.python.data.ops.dataset_ops.BatchDataset instance\n",
      "    Creates a `Dataset` with at most `count` elements from this dataset.\n",
      "    \n",
      "    >>> dataset = tf.data.Dataset.range(10)\n",
      "    >>> dataset = dataset.take(3)\n",
      "    >>> list(dataset.as_numpy_iterator())\n",
      "    [0, 1, 2]\n",
      "    \n",
      "    Args:\n",
      "      count: A `tf.int64` scalar `tf.Tensor`, representing the number of\n",
      "        elements of this dataset that should be taken to form the new dataset.\n",
      "        If `count` is -1, or if `count` is greater than the size of this\n",
      "        dataset, the new dataset will contain all elements of this dataset.\n",
      "    \n",
      "    Returns:\n",
      "      Dataset: A `Dataset`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Only use the 100 batches per epoch (that's 64 * 100 samples)\n",
    "model.fit(train_dataset.take(100), epochs=3)\n",
    "\n",
    "help(model.fit)\n",
    "help(train_dataset.take)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_e3b3rSmC-fL"
   },
   "source": [
    "#### 使用验证数据集\n",
    "<!--\n",
    "#### Using a validation dataset\n",
    "-->\n",
    "\n",
    "你可以为 `fit` 中的 `validation_data` 参数传入一个 Dataset 实例：\n",
    "<!--\n",
    "You can pass a Dataset instance as the `validation_data` argument in `fit`:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pv_53VE2C-11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps, validate for 157 steps\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3340 - sparse_categorical_accuracy: 0.9042 - val_loss: 0.1890 - val_sparse_categorical_accuracy: 0.9444\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1557 - sparse_categorical_accuracy: 0.9536 - val_loss: 0.1418 - val_sparse_categorical_accuracy: 0.9587\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1124 - sparse_categorical_accuracy: 0.9668 - val_loss: 0.1264 - val_sparse_categorical_accuracy: 0.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7e806d50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2Z7pgmSC7Zk"
   },
   "source": [
    "在每一个训练周期的末尾，模型都会对验证 Dataset 进行迭代，并计算验证损失函数和验证指标。\n",
    "<!--\n",
    "At the end of each epoch, the model will iterate over the validation Dataset and compute the validation loss and validation metrics.\n",
    "-->\n",
    "\n",
    "如果你只想使用该 Dataset 中一定数量的批次来进行验证，你可以传入 `validation_steps` 参数，该参数指定了模型在每一周期将会使用验证 Dataset 运行多少步，之后中止验证并继续下一周期的训练。\n",
    "<!--\n",
    "If you want to run validation only on a specific number of batches from this Dataset, you can pass the `validation_steps` argument, which specifies how many validation steps the model should run with the validation Dataset before interrupting validation and moving on to the next epoch:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGa4DQA0C8Mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps, validate for 10 steps\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3342 - sparse_categorical_accuracy: 0.9057 - val_loss: 0.3194 - val_sparse_categorical_accuracy: 0.9141\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1583 - sparse_categorical_accuracy: 0.9531 - val_loss: 0.2435 - val_sparse_categorical_accuracy: 0.9344\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1163 - sparse_categorical_accuracy: 0.9652 - val_loss: 0.2052 - val_sparse_categorical_accuracy: 0.9406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb97520dd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "# Prepare the training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Prepare the validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3,\n",
    "          # Only run validation using the first 10 batches of the dataset\n",
    "          # using the `validation_steps` argument\n",
    "          validation_data=val_dataset, validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qo-fWETC4sD"
   },
   "source": [
    "\n",
    "<!--\n",
    "Note that the Dataset is reset at the end of each epoch, so it can be reused of the next epoch.\n",
    "-->\n",
    "\n",
    "如果你只想使用该 Dataset 中一定数量的批次来进行训练，你可以传递 `steps_per_epoch` 参数，该参数指定了模型将在使用这个 Dataset 训练多少步之后再开始下一次训练周期（译注：即每个周期的训练步数）。\n",
    "<!--\n",
    "If you want to run training only on a specific number of batches from this Dataset, you can pass the `steps_per_epoch` argument, which specifies how many training steps the model should run using this Dataset before moving on to the next epoch.\n",
    "-->\n",
    "\n",
    "如果你这样做，数据集在每一个训练周期结束后将不会被重置，相反，我们将直接取用接下来的数据批次。最终，数据集里的数据将被用完（除非它是一个无限循环数据集）。\n",
    "<!--\n",
    "If you do this, the dataset is not reset at the end of each epoch, instead we just keep drawing the next batches. The dataset will eventually run out of data (unless it is an infinitely-looping dataset).\n",
    "-->\n",
    "\n",
    "注意，验证 Dataset 在每一次被使用之后都会被重置（因此在不同的周期，你都在基于相同的验证样本进行评估）。\n",
    "<!--\n",
    "Note that the validation Dataset will be reset after each use (so that you will always be evaluating on the same samples from epoch to epoch).\n",
    "-->\n",
    "\n",
    "使用 Dataset 对象进行训练时，不支持 `validation_split` 参数（该参数可以帮助我们从训练数据中生成维持数据集），因为该特性要求我们能够索引数据集的样本，而这通常是不能使用 Dataset API 做到的。\n",
    "<!--\n",
    "The argument `validation_split` (generating a holdout set from the training data) is not supported when training from Dataset objects, since this features requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIGt6NNqC2ZR"
   },
   "source": [
    "### 支持的其他输入格式\n",
    "<!--\n",
    "### Other input formats supported\n",
    "-->\n",
    "\n",
    "在 Numpy 数组和 TensorFlow Dataset 之外，我们还可以使用 Pandas dataframe 或可以生成（yield）数据批次的 Python 生成器来训练 Keras 模型。\n",
    "<!--\n",
    "Besides Numpy arrays and TensorFlow Datasets, it's possible to train a Keras model using Pandas dataframes, or from Python generators that yield batches.\n",
    "-->\n",
    "\n",
    "通常来说，如果你的数据量比较小，能够在内存中储存下来，我们建议使用 Numpy 输入数据；否则请使用 Dataset。\n",
    "<!--\n",
    "In general, we recommend that you use Numpy input data if your data is small and fits in memory, and Datasets otherwise.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bid1XnTl_zNT"
   },
   "source": [
    "### 使用样本加权和类别加权\n",
    "<!--\n",
    "### Using sample weighting and class weighting\n",
    "-->\n",
    "\n",
    "在使用 `fit` 时，除了输入数据和目标数据，我们还可以为模型传入样本权重或者类别权重：\n",
    "<!--\n",
    "Besides input data and target data, it is possible to pass sample weights or class weights to a model when using `fit`:\n",
    "-->\n",
    "\n",
    "- 在使用 Numpy 数据训练时：请使用 `sample_weight` 和 `class_weight` 参数。\n",
    "- 在使用 Dataset 训练时：请让你的 Dataset 返回一个格式为 `(input_batch, target_batch, sample_weight_batch)` 的数组。\n",
    "\n",
    "<!--\n",
    "- When training from Numpy data: via the `sample_weight` and `class_weight` arguments.\n",
    "- When training from Datasets: by having the Dataset return a tuple `(input_batch, target_batch, sample_weight_batch)` .\n",
    "-->\n",
    "\n",
    "“sample weights”数组给定了在计算总损失时一个批次中的每个样本应当具有的权重。它通常用于非平衡分类问题（imbalanced classification problems；即给不常见的类别赋予更高的权重）。若权重值非 0 即 1，那么该权重数组可以用作损失函数的*掩码*（*mask*；即整体地丢弃某些样本对于总损失的贡献）。\n",
    "<!--\n",
    "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes). When the weights used are ones and zeros, the array can be used as a *mask* for the loss function (entirely discarding the contribution of certain samples to the total loss).\n",
    "-->\n",
    "\n",
    "“class weights”字典代表了同样的概念，但其含义更加明确：它将类别序号映射到那些属于该类别的样本应当具有的权重值。例如，如果你的数据中，类别“0”的数量是类别“1”的一半，那么你可以使用 `class_weight={0: 1., 1: 0.5}`。\n",
    "<!--\n",
    "A \"class weights\" dict is a more specific instance of the same concept: it maps class indices to the sample weight that should be used for samples belonging to this class. For instance, if class \"0\" is twice less represented than class \"1\" in your data, you could use `class_weight={0: 1., 1: 0.5}`.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbH4jT5Hp_gg"
   },
   "source": [
    "下面这个例子使用了 Numpy，我们利用类别权重或者样本权重来提升正确分类类别 #5（即 MNIST 数据集中的数字“5”）的重要性。\n",
    "<!--\n",
    "Here's a Numpy example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Y7QBNUXWTva"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit with class weight\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 1s 30us/sample - loss: 0.1045 - sparse_categorical_accuracy: 0.9707\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.0857 - sparse_categorical_accuracy: 0.9764\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 1s 24us/sample - loss: 0.0739 - sparse_categorical_accuracy: 0.9792\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 1s 25us/sample - loss: 0.0640 - sparse_categorical_accuracy: 0.9816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7bc1c910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
    "                # Set weight \"2\" for class \"5\",\n",
    "                # making this class 2x more important\n",
    "                5: 2.,\n",
    "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
    "print('Fit with class weight')\n",
    "model.fit(x_train, y_train,\n",
    "          class_weight=class_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suvk2extDq0Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fit with sample weight\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 50000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 2s 35us/sample - loss: 0.3813 - sparse_categorical_accuracy: 0.9005\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 1s 26us/sample - loss: 0.1727 - sparse_categorical_accuracy: 0.9514\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 1s 27us/sample - loss: 0.1266 - sparse_categorical_accuracy: 0.9637\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 1s 28us/sample - loss: 0.1021 - sparse_categorical_accuracy: 0.9710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb98c03e90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's the same example using `sample_weight` instead:\n",
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "print('\\nFit with sample weight')\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(x_train, y_train,\n",
    "          sample_weight=sample_weight,\n",
    "          batch_size=64,\n",
    "          epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qf1KvM0pqBaV"
   },
   "source": [
    "下面是匹配 Dataset 的例子：\n",
    "<!--\n",
    "Here's a matching Dataset example:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHD60GshxSpf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 782 steps\n",
      "Epoch 1/3\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.3802 - sparse_categorical_accuracy: 0.9008\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1781 - sparse_categorical_accuracy: 0.9504\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 1s 2ms/step - loss: 0.1305 - sparse_categorical_accuracy: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7e917810>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight = np.ones(shape=(len(y_train),))\n",
    "sample_weight[y_train == 5] = 2.\n",
    "\n",
    "# Create a Dataset that includes sample weights\n",
    "# (3rd element in the return tuple).\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train, sample_weight))\n",
    "\n",
    "# Shuffle and slice the dataset.\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXt0xQvDxMeq"
   },
   "source": [
    "### 将数据传递给多输入、多输出的模型\n",
    "<!--\n",
    "### Passing data to multi-input, multi-output models\n",
    "-->\n",
    "\n",
    "在上面的例子中，我们考虑的是只有一个输入（形状为 `(764,)` 的张量）且只有一个输出（形状为 `(10,)` 的预测张量）的模型。那么对于有着多个输入和多个输出的模型我们该怎么办呢？\n",
    "<!--\n",
    "In the previous examples, we were considering a model with a single input (a tensor of shape `(764,)`) and a single output (a prediction tensor of shape `(10,)`). But what about models that have multiple inputs or outputs?\n",
    "-->\n",
    "\n",
    "考虑如下模型，它有一个形状为 `(32, 32, 3)` 的图像输入（其形状表示图像的 `(height, width, channels)`），以及一个形状为 `(None, 10)` 的时间序列输入（其形状表示 `(timesteps, features)`）。我们的模型将会根据这些输入计算出两个输出：一个“评分”（形状为 `(1,)`），以及一个关于五个类别的概率分布（形状为 `(5,)`）。\n",
    "<!--\n",
    "Consider the following model, which has an image input of shape `(32, 32, 3)` (that's `(height, width, channels)`) and a timeseries input of shape `(None, 10)` (that's `(timesteps, features)`). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape `(1,)`) and a probability distribution over five classes (of shape `(5,)`).\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNUSGfKq1cZ-"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
    "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
    "\n",
    "x1 = layers.Conv2D(3, 3)(image_input)\n",
    "x1 = layers.GlobalMaxPooling2D()(x1)\n",
    "\n",
    "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
    "x2 = layers.GlobalMaxPooling1D()(x2)\n",
    "\n",
    "x = layers.concatenate([x1, x2])\n",
    "\n",
    "score_output = layers.Dense(1, name='score_output')(x)\n",
    "class_output = layers.Dense(5, name='class_output')(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, timeseries_input],\n",
    "                    outputs=[score_output, class_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cIWhMd7p3NGp"
   },
   "source": [
    "让我们绘制这个模型，这样你就能清楚地知道我们这是在做什么了（注意图片中显示的形状是批次的形状，而不是每个样本的形状）。\n",
    "<!--\n",
    "Let's plot this model, so you can clearly see what we're doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes).\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKVQ4Y573Q_c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAauLxiT278G"
   },
   "source": [
    "在编译时，我们可以为不同的输出指定不同的损失函数，只须将损失函数以列表形式传入即可：\n",
    "<!--\n",
    "At compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDa7JSz93phE"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy(from_logits=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQip1Fex4-D5"
   },
   "source": [
    "如果我们只为模型传入了一个损失函数，那么它将应用于每个输出，在现在这个例子中这样做并不可取。\n",
    "<!--\n",
    "If we only passed a single loss function to the model, the same loss function would be applied to every output, which is not appropriate here.\n",
    "-->\n",
    "\n",
    "对于指标，方法类似：\n",
    "<!--\n",
    "Likewise for metrics:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tkl5LMGi4_gK"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy(from_logits=True)],\n",
    "    metrics=[[keras.metrics.MeanAbsolutePercentageError(),\n",
    "              keras.metrics.MeanAbsoluteError()],\n",
    "             [keras.metrics.CategoricalAccuracy()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_WWk9b1374P"
   },
   "source": [
    "因为我们已经为每个输出层取了名字，所以我们也可以使用字典来指定每个输出的损失函数和指标：\n",
    "<!--\n",
    "Since we gave names to our output layers, we could also specify per-output losses and metrics via a dict:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1MEctLBD4APc"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy(from_logits=True)},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NVXRX0PX6lIn"
   },
   "source": [
    "如果你有 2 个以上的输出，我们建议你显式地为输出层命名并使用字典来指定每个输出的损失函数和指标。\n",
    "<!--\n",
    "We recommend the use of explicit names and dicts if you have more than 2 outputs.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RX3FTDWw4BOA"
   },
   "source": [
    "我们还可以为不同输出的损失函数赋予不同的权重（例如，我们希望将上述例子中“score”的损失函数的重要性提升至“class”的损失函数的两倍），只须使用 `loss_weights` 参数：\n",
    "<!--\n",
    "It's possible to give different weights to different output-specific losses (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), using the `loss_weights` argument:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mgVK3xAv4JNv"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
    "          'class_output': keras.losses.CategoricalCrossentropy(from_logits=True)},\n",
    "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
    "                              keras.metrics.MeanAbsoluteError()],\n",
    "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
    "    loss_weights={'score_output': 2., 'class_output': 1.})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vd8VzIu-3p0n"
   },
   "source": [
    "你也可以选择不计算某些输出的损失函数，如果这些输出仅仅表示预测而不会用来训练模型的话：\n",
    "<!--\n",
    "You could also chose not to compute a loss for certain outputs, if these outputs meant for prediction but not for training:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aUyohARI4Kqn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output score_output missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to score_output.\n"
     ]
    }
   ],
   "source": [
    "# List loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[None, keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Or dict loss version\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={'class_output':keras.losses.CategoricalCrossentropy(from_logits=True)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0MwZc184MG_"
   },
   "source": [
    "使用 `fit` 来为多输入多输出模型传入数据，方法和在 `compile` 中指定损失函数一样：你可以传入 *Numpy 数组的列表（输出数据应当和接受了损失函数的输出层一一对应）*，或者是*以输入/输出层名字为键、以相应 Numpy 数组训练数据为值的字典*。\n",
    "<!--\n",
    "Passing data to a multi-input or multi-output model in `fit` works in a similar way as specifying a loss function in `compile`:\n",
    "you can pass *lists of Numpy arrays (with 1:1 mapping to the outputs that received a loss function)* or *dicts mapping output names to Numpy arrays of training data*.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "So4cYwSW4la8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 1s 7ms/sample - loss: 6.5601 - score_output_loss: 0.8151 - class_output_loss: 5.5315\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 279us/sample - loss: 6.0020 - score_output_loss: 0.4939 - class_output_loss: 5.1231\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 264us/sample - loss: 5.7812 - score_output_loss: 0.2978 - class_output_loss: 5.3716\n",
      "Train on 100 samples\n",
      "Epoch 1/3\n",
      "100/100 [==============================] - 0s 258us/sample - loss: 5.6348 - score_output_loss: 0.2021 - class_output_loss: 5.3871\n",
      "Epoch 2/3\n",
      "100/100 [==============================] - 0s 233us/sample - loss: 5.5357 - score_output_loss: 0.1273 - class_output_loss: 5.1933\n",
      "Epoch 3/3\n",
      "100/100 [==============================] - 0s 296us/sample - loss: 5.4635 - score_output_loss: 0.1163 - class_output_loss: 5.3535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7f3305d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[keras.losses.MeanSquaredError(),\n",
    "          keras.losses.CategoricalCrossentropy(from_logits=True)])\n",
    "\n",
    "# Generate dummy Numpy data\n",
    "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
    "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
    "score_targets = np.random.random_sample(size=(100, 1))\n",
    "class_targets = np.random.random_sample(size=(100, 5))\n",
    "\n",
    "# Fit on lists\n",
    "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
    "          batch_size=32,\n",
    "          epochs=3)\n",
    "\n",
    "# Alternatively, fit on dicts\n",
    "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
    "          {'score_output': score_targets, 'class_output': class_targets},\n",
    "          batch_size=32,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7Ah1QUf4ll6"
   },
   "source": [
    "如果使用 Dataset，情况和 Numpy 数组类似，Dataset 应当返回一个由字典组成的元组。\n",
    "<!--\n",
    "Here's the Dataset use case: similarly as what we did for Numpy arrays, the Dataset should return\n",
    "a tuple of dicts.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vx34-FDZ4tdJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2 steps\n",
      "Epoch 1/3\n",
      "2/2 [==============================] - 0s 243ms/step - loss: 5.4280 - score_output_loss: 0.0962 - class_output_loss: 5.3242\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 5.4253 - score_output_loss: 0.0956 - class_output_loss: 5.3266\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 5.4593 - score_output_loss: 0.0936 - class_output_loss: 5.3800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7e1dc290>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'img_input': img_data, 'ts_input': ts_data},\n",
    "     {'score_output': score_targets, 'class_output': class_targets}))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "model.fit(train_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_gqVFcHAAga"
   },
   "source": [
    "### 使用回调函数\n",
    "<!--\n",
    "### Using callbacks\n",
    "-->\n",
    "\n",
    "在 Keras 中，回调函数是在训练过程的不同时间点（如在训练周期的开始，在训练完一个批次之后，或者在训练完一个周期之后，等等）被调用的对象，它们能用来完成如下任务：\n",
    "<!--\n",
    "Callbacks in Keras are objects that are called at different point during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.) and which can be used to implement behaviors such as:\n",
    "-->\n",
    "\n",
    "- 在训练的不同时间点进行验证（而不仅限于内置的每周期验证）\n",
    "- 每隔一段时间或者在模型达到某个精度阈值后为模型保存[检查点](https://developers.google.cn/machine-learning/glossary/?hl=zh-CN#%E6%A3%80%E6%9F%A5%E7%82%B9-checkpoint)\n",
    "- 在训练可能陷入稳定区域（plateau）时调整模型的学习率\n",
    "- 在训练可能陷入稳定区域（plateau）时为顶部的层进行微调\n",
    "- 训练结束后或者模型达到某个准确度阈值时自动发送邮件或者即时消息提醒\n",
    "- 等等\n",
    "\n",
    "<!--\n",
    "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
    "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
    "- Changing the learning rate of the model when training seems to be plateauing\n",
    "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
    "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded\n",
    "- Etc.\n",
    "-->\n",
    "\n",
    "在你调用 `fit` 时，你可以将回调函数以列表形式传入：\n",
    "<!--\n",
    "Callbacks can be passed as a list to your call to `fit`:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNEhXWcnSG9B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.3644 - sparse_categorical_accuracy: 0.8984 - val_loss: 0.2320 - val_sparse_categorical_accuracy: 0.9316\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.1628 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.1639 - val_sparse_categorical_accuracy: 0.9511\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.1184 - sparse_categorical_accuracy: 0.9647 - val_loss: 0.1583 - val_sparse_categorical_accuracy: 0.9533\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 1s 26us/sample - loss: 0.0943 - sparse_categorical_accuracy: 0.9721 - val_loss: 0.1452 - val_sparse_categorical_accuracy: 0.9583\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.0785 - sparse_categorical_accuracy: 0.9761 - val_loss: 0.1359 - val_sparse_categorical_accuracy: 0.9621\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.0656 - sparse_categorical_accuracy: 0.9811 - val_loss: 0.1352 - val_sparse_categorical_accuracy: 0.9624\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffb7ff35e10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor='val_loss',\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\" \n",
    "        # (即：和上一周期的值相比减少不超过 1e-2)\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=20,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE3VK3WpSHL_"
   },
   "source": [
    "#### 有很多内置回调函数可以使用\n",
    "<!--\n",
    "#### Many built-in callbacks are available\n",
    "-->\n",
    "\n",
    "- `ModelCheckpoint`：按周期保存模型。\n",
    "- `EarlyStopping`：在训练过程中验证指标不再得到优化后停止训练。\n",
    "- `TensorBoard`：周期性地将模型日志记录下来，以便在 TensorBoard 可视化查看（更多细节参见“可视化”一节）。\n",
    "- `CSVLogger`：将损失和指标数据流式存储至 CSV 文件中。\n",
    "- 等等。\n",
    "\n",
    "<!--\n",
    "- `ModelCheckpoint`: Periodically save the model.\n",
    "- `EarlyStopping`: Stop training when training is no longer improving the validation metrics.\n",
    "- `TensorBoard`: periodically write model logs that can be visualized in TensorBoard (more details in the section \"Visualization\").\n",
    "- `CSVLogger`: streams loss and metrics data to a CSV file.\n",
    "- etc.\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "#### 编写你自己的回调函数\n",
    "<!--\n",
    "#### Writing your own callback\n",
    "-->\n",
    "\n",
    "你可以通过继承基类 keras.callbacks.Callback 来创建自定义回调函数。一个回调函数可以通过类属性 `self.model` 访问其关联的模型。\n",
    "<!--\n",
    "You can create a custom callback by extending the base class keras.callbacks.Callback. A callback has access to its associated model through the class property `self.model`.\n",
    "-->\n",
    "\n",
    "下面这个例子会保存训练期间每个批次的损失函数值组成的列表：\n",
    "<!--\n",
    "Here's a simple example saving a list of per-batch loss values during training:\n",
    "-->\n",
    "\n",
    "```python\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mC08S-e-yOd7"
   },
   "source": [
    "### 为模型创建检查点\n",
    "<!--\n",
    "### Checkpointing models\n",
    "-->\n",
    "\n",
    "当你在相当大的数据集上训练模型时，较频繁地为你的模型保存检查点是相当重要的。\n",
    "<!--\n",
    "When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.\n",
    "-->\n",
    "\n",
    "最简单的方法是使用 `ModelCheckpoint` 回调：\n",
    "<!--\n",
    "The easiest way to achieve this is with the `ModelCheckpoint` callback:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yEER-qc-xXNC"
   },
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='mymodel_{epoch}',\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        verbose=1)\n",
    "]\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=3,\n",
    "          batch_size=64,\n",
    "          callbacks=callbacks,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1AZEf3ixXm8"
   },
   "source": [
    "你也可以编写你自己的回调，以便保存和恢复模型。\n",
    "<!--\n",
    "You call also write your own callback for saving and restoring models.\n",
    "-->\n",
    "\n",
    "关于模型序列化和保存的完整指南，请参见[模型序列化和模型保存的指南](./save_and_serialize.ipynb)。\n",
    "<!--\n",
    "For a complete guide on serialization and saving, see [Guide to Saving and Serializing Models](./save_and_serialize.ipynb).\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j22FtN-z_-Wj"
   },
   "source": [
    "### 使用学习率计划\n",
    "<!--\n",
    "### Using learning rate schedules\n",
    "-->\n",
    "\n",
    "在训练深度学习模型时，一个常见的模式是随着训练的推进，逐渐减小学习率。这通常被称为“学习率衰减”。\n",
    "<!--\n",
    "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as \"learning rate decay\".\n",
    "-->\n",
    "\n",
    "学习率衰减计划可以是静态的（被提前确定好，学习率是训练周期编号或者批次编号的函数），也可以是动态的（学习率会依据当前模型的表现来调整，尤其是会考虑验证损失值）。\n",
    "<!--\n",
    "The learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss).\n",
    "-->\n",
    "\n",
    "#### 为优化器传入学习率计划\n",
    "<!--\n",
    "#### Passing a schedule to an optimizer\n",
    "-->\n",
    "\n",
    "你可以很容易地使用静态学习率衰减计划，只须在你的优化器中将计划对象作为 `learning_rate` 参数传入：\n",
    "<!--\n",
    "You can easily use a static learning rate decay schedule by passing a schedule object as the `learning_rate` argument in your optimizer:\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu-pnB_ctEav"
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ltf0Xtrtw-2"
   },
   "source": [
    "若干内置计划可以使用：`ExponentialDecay`，`PiecewiseConstantDecay`，`PolynomialDecay`，以及 `InverseTimeDecay`。\n",
    "<!--\n",
    "Several built-in schedules are available: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay`, and `InverseTimeDecay`.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13CvYhBptEjh"
   },
   "source": [
    "#### 使用回调来实现动态学习率计划\n",
    "<!--\n",
    "#### Using callbacks to implement a dynamic learning rate schedule\n",
    "-->\n",
    "\n",
    "动态学习率计划（例如，在验证损失不再降低时减少学习率）不能通过上述计划对象实现，因为优化器不能获取到验证指标。\n",
    "<!--\n",
    "A dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects since the optimizer does not have access to validation metrics.\n",
    "-->\n",
    "\n",
    "然而，回调是可以获取到所有指标的，包括验证指标。因此，要想达到这个目的，你可以使用回调来更改优化器的学习率。事实上，该功能已经被内置为 `ReduceLROnPlateau` 回调。\n",
    "<!--\n",
    "However, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the `ReduceLROnPlateau` callback.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnUhGslABdvx"
   },
   "source": [
    "### 在训练期间可视化损失函数和指标\n",
    "<!--\n",
    "### Visualizing loss and metrics during training\n",
    "-->\n",
    "\n",
    "在训练时，能够时刻紧盯你的模型的最好方法是使用 [TensorBoard](https://www.tensorflow.org/tensorboard)，这是一个可以在本地运行的、基于浏览器的应用，它让你可以：\n",
    "<!--\n",
    "The best way to keep an eye on your model during training is to use [TensorBoard](https://www.tensorflow.org/tensorboard), a browser-based application that you can run locally that provides you with:\n",
    "-->\n",
    "\n",
    "- 为训练和评估实时绘制损失和指标曲线\n",
    "- （可选）为每层的激活函数绘制直方图\n",
    "- （可选）为你的 `Embedding` 层学习到的嵌套空间进行 3D 可视化\n",
    "<!--\n",
    "- Live plots of the loss and metrics for training and evaluation\n",
    "- (optionally) Visualizations of the histograms of your layer activations\n",
    "- (optionally) 3D visualizations of the embedding spaces learned by your `Embedding` layers\n",
    "-->\n",
    "\n",
    "如果你是使用 pip 安装 TensorFlow 的，你可以在命令行中启动 TensorBoard：\n",
    "<!--\n",
    "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
    "-->\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```\n",
    "\n",
    "#### 使用 TensorBoard 回调\n",
    "<!--\n",
    "#### Using the TensorBoard callback\n",
    "-->\n",
    "\n",
    "通过 Keras 模型和 `fit` 方法使用 TensorBoard 最容易的方法是使用 `TensorBoard` 回调。\n",
    "<!--\n",
    "The easiest way to use TensorBoard with a Keras model and the `fit` method is the `TensorBoard` callback.\n",
    "-->\n",
    "\n",
    "最简单的情形只须指定你希望回调将日志保存在何处，即：\n",
    "<!--\n",
    "In the simplest case, just specify where you want the callback to write logs, and you're good to go:\n",
    "-->\n",
    "\n",
    "```python\n",
    "tensorboard_cbk = keras.callbacks.TensorBoard(log_dir='/full_path_to_your_logs')\n",
    "model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk])\n",
    "```\n",
    "\n",
    "`TensorBoard` 回调有很多有用的选项，包括是否记录嵌套、直方图，以及保存日志的频率：\n",
    "<!--\n",
    "The `TensorBoard` callback has many useful options, including whether to log embeddings, histograms, and how often to write logs:\n",
    "-->\n",
    "\n",
    "```python\n",
    "keras.callbacks.TensorBoard(\n",
    "    log_dir='/full_path_to_your_logs',\n",
    "    histogram_freq=0,  # How often to log histogram visualizations\n",
    "    embeddings_freq=0,  # How often to log embedding visualizations\n",
    "    update_freq='epoch')  # How often to write logs (default: once per epoch)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5r5ZnFry7-B7"
   },
   "source": [
    "## 第二部分：从零开始编写你自己的训练和评估循环\n",
    "<!--\n",
    "## Part II: Writing your own training & evaluation loops from scratch\n",
    "-->\n",
    "\n",
    "如果你想让你的训练和评估循环比 `fit()` 和 `evaluate()` 所提供的更加底层，你应当自己动手编写。实际上这非常简单！不过你需要自己做更多的 debug。\n",
    "<!--\n",
    "If you want lower-level over your training & evaluation loops than what `fit()` and `evaluate()` provide, you should write your own. It's actually pretty simple! But you should be ready to have a lot more debugging to do on your own.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HaRvt057_37D"
   },
   "source": [
    "### 使用 GradientTape：第一个端到端的例子\n",
    "<!--\n",
    "### Using the GradientTape: a first end-to-end example\n",
    "-->\n",
    "\n",
    "在 `GradientTape` 内调用一个模型，你将可以得到损失函数关于层的可训练参数的梯度。使用优化器实例，你可以利用这些梯度值来更行这些可训练参数的数值（这些可训练参数可以通过 `model.trainable_weights` 得到）。\n",
    "<!--\n",
    "Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of the trainable weights of the layer with respect to a loss value. Using an optimizer instance, you can use these gradients to update these variables (which you can retrieve using `model.trainable_weights`).\n",
    "-->\n",
    "\n",
    "让我们继续使用第一部分最开始的 MNIST 模型，并在训练循环中使用小批次梯度来训练它。\n",
    "<!--\n",
    "Let's reuse our initial MNIST model from Part I, and let's train it using mini-batch gradient with a training loop.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQqxREEL66Kg"
   },
   "outputs": [],
   "source": [
    "# Get the model.\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sD9ewUX_Y7oY"
   },
   "source": [
    "训练若干周期：\n",
    "<!--\n",
    "Run a training loop for a few epochs:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkKmTnbRY-Ie"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.318831443786621\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.2238218784332275\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.1913812160491943\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.0636324882507324\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.003420829772949\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.873820185661316\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.9026579856872559\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.688047170639038\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.6979117393493652\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.5427498817443848\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.5922415256500244\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.3910906314849854\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                'Training loss (for one batch) at step %s: %s' % \n",
    "                (step, float(loss_value))\n",
    "            )\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lt9Yva8xgCs"
   },
   "source": [
    "### 指标的底层操作\n",
    "<!--\n",
    "### Low-level handling of metrics\n",
    "-->\n",
    "\n",
    "让我们接着添加指标。在上面这样从零开始编写的训练循环中，你仍然可以使用内置的（或者是你自定义的）指标。具体过程如下：\n",
    "<!--\n",
    "Let's add metrics to the mix. You can readily reuse the built-in metrics (or custom ones you wrote) in such training loops written from scratch. Here's the flow:\n",
    "-->\n",
    "\n",
    "- 在循环开始时初始化指标\n",
    "- 在每批次训练完后调用 `metric.update_state()`\n",
    "- 在你需要显示指标当前的值时，调用 `metric.result()`\n",
    "- 在你需要清除指标的状态时（通常是在一个周期结束后），调用 `metric.reset_states()`\n",
    "\n",
    "<!--\n",
    "- Instantiate the metric at the start of the loop\n",
    "- Call `metric.update_state()` after each batch\n",
    "- Call `metric.result()` when you need to display the current value of the metric\n",
    "- Call `metric.reset_states()` when you need to clear the state of the metric (typically at the end of an epoch)\n",
    "-->\n",
    "\n",
    "基于这些知识，让我们在每个周期结束后在验证数据集上计算 `SparseCategoricalAccuracy`：\n",
    "<!--\n",
    "Let's use this knowledge to compute `SparseCategoricalAccuracy` on validation data at the end of each epoch:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDKgp70UxwdR"
   },
   "outputs": [],
   "source": [
    "# Get model\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Prepare the metrics.\n",
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Prepare the training dataset.\n",
    "batch_size = 64\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "# Prepare the validation dataset.\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-0mCCz4Yr2f"
   },
   "source": [
    "训练若干周期：\n",
    "<!--\n",
    "Run a training loop for a few epochs:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA5RPNT0YpZu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 2.322082996368408\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.1729562282562256\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.1619951725006104\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.1269915103912354\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.289000004529953\n",
      "Validation acc: 0.45239999890327454\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.0694031715393066\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.9236276149749756\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.8952229022979736\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.7053300142288208\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.5364800095558167\n",
      "Validation acc: 0.6445000171661377\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.6053411960601807\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 1.4380018711090088\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 1.4677841663360596\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 1.356065273284912\n",
      "Seen so far: 38464 samples\n",
      "Training acc over epoch: 0.6748999953269958\n",
      "Validation acc: 0.7458000183105469\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Update training metric.\n",
    "        train_acc_metric(y_batch_train, logits)\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                'Training loss (for one batch) at step %s: %s' % \n",
    "                (step, float(loss_value))\n",
    "            )\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))\n",
    "\n",
    "    # Display metrics at the end of each epoch.\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print('Training acc over epoch: %s' % (float(train_acc),))\n",
    "    # Reset training metrics at the end of each epoch\n",
    "    train_acc_metric.reset_states()\n",
    "\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    for x_batch_val, y_batch_val in val_dataset:\n",
    "        val_logits = model(x_batch_val)\n",
    "        # Update val metrics\n",
    "        val_acc_metric(y_batch_val, val_logits)\n",
    "    val_acc = val_acc_metric.result()\n",
    "    val_acc_metric.reset_states()\n",
    "    print('Validation acc: %s' % (float(val_acc),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjraqJAonBJK"
   },
   "source": [
    "### 对额外损失函数的底层操作\n",
    "<!--\n",
    "### Low-level handling of extra losses\n",
    "-->\n",
    "\n",
    "在前一节中，你已经知道我们可以通过在层的 `call` 方法中调用 `self.add_loss(value)` 来附加正则化损失。\n",
    "<!--\n",
    "You saw in the previous section that it is possible for regularization losses to be added by a layer by calling `self.add_loss(value)` in the `call` method.\n",
    "-->\n",
    "\n",
    "一般而言，你需要在训练循环中将这些损失考虑在内（除非你自己编写模型，并且你已经知道该模型不存在这种损失）。\n",
    "<!--\n",
    "In the general case, you will want to take these losses into account in your training loops (unless you've written the model yourself and you already know that it creates no such losses).\n",
    "-->\n",
    "\n",
    "回忆一下上一节中的这个例子（它定义了一个计算正则化损失的层）：\n",
    "<!--\n",
    "Recall this example from the previous section, featuring a layer that creates a regularization loss:\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fglnb_wzwtxh"
   },
   "outputs": [],
   "source": [
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
    "        return inputs\n",
    "\n",
    "inputs = keras.Input(shape=(784,), name='digits')\n",
    "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "# Insert activity regularization as a layer\n",
    "x = ActivityRegularizationLayer()(x)\n",
    "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "outputs = layers.Dense(10, name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44zwXYXdwwss"
   },
   "source": [
    "当你像这样调用一个模型时：\n",
    "<!--\n",
    "When you call a model, like this:\n",
    "-->\n",
    "\n",
    "```python\n",
    "logits = model(x_train)\n",
    "```\n",
    "\n",
    "上述正则化层在前向传播过程中带来的损失将被加到 `model.losses` 属性上：\n",
    "<!--\n",
    "the losses it creates during the forward pass are added to the `model.losses` attribute:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOWuXgDlxIbS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=7.7824626>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train[:64])\n",
    "print(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvkOERFYymF_"
   },
   "source": [
    "模型记录的损失在调用其 `__call__` 方法时将首先被重置，因此你只能看到在这*一次*前向传播中产生的损失。例如，反复调用模型，然后查询 `losses`，程序将只会显示最后的损失函数值，也就是最后一次调用过程产生的损失值：\n",
    "<!--\n",
    "The tracked losses are first cleared at the start of the model `__call__`, so you will only see the losses created during this one forward pass. For instance, calling the model repeatedly and then querying `losses` only displays the latest losses, created during the last call:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZh9ek_NzEtQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=7.685816>]\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train[:64])\n",
    "logits = model(x_train[64: 128])\n",
    "logits = model(x_train[128: 192])\n",
    "print(model.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MmftRMi6zWLN"
   },
   "source": [
    "要想在训练过程中将这些损失全部考虑在内，你需要做的就是修改你的训练循环，将 `sum(model.losses)` 加到你的总损失中：\n",
    "<!--\n",
    "To take these losses into account during training, all you have to do is to modify your training loop to add `sum(model.losses)` to your total loss:\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueLhIG1mzdFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 9.751727104187012\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.4815170764923096\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.39096736907959\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.354135274887085\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.3341073989868164\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.332087278366089\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.324887275695801\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.30796480178833\n",
      "Seen so far: 38464 samples\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 2.3236911296844482\n",
      "Seen so far: 64 samples\n",
      "Training loss (for one batch) at step 200: 2.313704490661621\n",
      "Seen so far: 12864 samples\n",
      "Training loss (for one batch) at step 400: 2.3197813034057617\n",
      "Seen so far: 25664 samples\n",
      "Training loss (for one batch) at step 600: 2.3123652935028076\n",
      "Seen so far: 38464 samples\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "            # Add extra losses created during this forward pass:\n",
    "            loss_value += sum(model.losses)\n",
    "\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                'Training loss (for one batch) at step %s: %s' % \n",
    "                (step, float(loss_value))\n",
    "            )\n",
    "            print('Seen so far: %s samples' % ((step + 1) * 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCOe4e4dxsB0"
   },
   "source": [
    "这便是拼图的最后一块！你已经来到了本指南的最后。\n",
    "<!--\n",
    "That was the last piece of the puzzle! You've reached the end of this guide.\n",
    "-->\n",
    "\n",
    "现在你已经知道了使用内置训练循环和自己从零开始编写训练循环的方法~\n",
    "<!--\n",
    "Now you know everything there is to know about using built-in training loops and writing your own from scratch.\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_and_evaluate.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
